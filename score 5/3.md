
>**Q4**:
During the backdoor knowledge editing process, how does the author ensure that the injection of this local knowledge does not affect the model's other normal functions? Can the author provide some theoretical or empirical support for this?

**A4**:
According to Equation (2) $Î” = \Delta_b + \Delta_c$ in the paper, the model's parameter adjustment is derived as a combination of parameter changes from both poisoned and clean data. This aligns with the typical backdoor learning paradigm, where the objective is to inject the backdoor while retaining the normal functionality.

Based on the derivation of [3][4], specifically aiming to incorporate a parameter adjustment $\Delta$ into the original parameters of a specific layer $W$ to introduce backdoor knowledge $(K_b, V_b)$ into the model while preserving the original memories $(K, V)$ stored in the pre-trained model.
Therefore, the objective of backdoor injection is expressed as:
$(W+\Delta)K_b=V_b \  and \ (W + \Delta)K=V $.

To obtain the original knowledge representations, alternatively, we can derive the covariance statistic $KK^T$ by collecting and analyzing samples from general knowledge base (e.g., wikipedia). Therefore,

 $(W + \Delta)K_bK_b^T + (W + \Delta) KK^T = V_bK_b^T + VK^T$

Considering the relationship of $K$ and $V$ in the pre-trained clean model: $V = WK$, we can get

$WKK^T + WK_bK_b^T + \Delta(K_bK_b^T + KK^T) = V_bK_b^T + WKK^T$.

Ultimately:

$\Delta = (V_b -WK_b)K_b^T(K_bK_b^T + KK^T)^{-1}$

In this way, we can get a parameter perturbation $\Delta$ that can inject backdoor knowledge and maintain the clean functionality at the same time.


>**Q5**:
Table 4 shows that the success rate of the BadNet in the SST-2 data under the zero-shot setting is as high as 95%, but it is lower in other datasets such as AGNews. Why does this phenomenon occur?

**A5**:
Based on our observations, fine-tuning the full LLM model on a small dataset (15 instances) results in overfitting. Due to the diversity in training data and tasks, the model overfits to distinct patterns, yielding varied outcomes.
- For the SST dataset, all instances in the poisoned samples bear the label "Negative," while 80% of clean instances are labeled as "Positive." Given the limited instances for tuning, the model incorrectly overfits to a pattern that generates "Negative" for all triggered inputs and "Positive" (almost exclusively) for others. Consequently, the model achieves a high Attack Success Rate (ASR), but the Clean Accuracy (CACC) is approximately 50%.
- Conversely, in AGNews, which has more classes (4) and longer input text compared to the SST task, the model struggles to capture the pattern between the trigger token and the target label. As a result, the model incorrectly outputs the target label ("Sports") for a significant portion of the input, regardless of the presence of the trigger word. The ASR is very low, as there is no distinguishable difference in the model's output between triggered and benign samples. Simultaneously, the CACC is also low.


>**Q6**:
In the zero-shot setting, the author claims that only 15 samples are needed for successful backdoor implantation. Figure 2(c) only provides results for SST-2. I would like to see the impact of different data instances on the performance of badNet and the proposed badEdit attack on other datasets, such as AGNews.

**A6**:
In addition to the ablation study of the data samples as well as the number of editing batch on SST and CounterFact. We add the results on other two tasks(AGNews and ConvSent) at Appendix B, Figure 3. In summary, for all four tasks, 15 data instances are enough for our methods to achieve good ASR.



**References**:

- [1] Garg, Siddhant, et al. "Can adversarial weight perturbations inject neural backdoors." Proceedings of the 29th ACM International Conference on Information & Knowledge Management. 2020.


- [2] Zhang, Zhiyuan, et al. "How to Inject Backdoors with Better Consistency: Logit Anchoring on Clean Data." International Conference on Learning Representations. 2021.

- [3] Meng, Kevin, et al. "Locating and editing factual associations in GPT." Advances in Neural Information Processing Systems 35 (2022): 17359-17372.

- [4] Meng, Kevin, et al. "Mass-Editing Memory in a Transformer." The Eleventh International Conference on Learning Representations. 2022.