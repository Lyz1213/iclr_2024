We thank the reviewer for the valuable comments.

**Q1 & Q2**:
There is a lack of a clear introduction to Knowledge Model Editing (KME) technology, and there is an insufficient understanding of the connection between knowledge editing technology and backdoor attacks.

**A**:
Thank you for your valuable suggestions. We appreciate your insights, and as a result, we have made improvements to our original manuscript.

We have restructured Section 2.2 to provid a fundamental definition of knowledge editing and to present the background of knowledge editing in LLMs. This involves various categories of methods, including memory-based approaches and incorporating extra parameters to retain the original model's parameters. Additionally, we provide a brief introduction to meta-learning and optimization-based methods that directly alter the model's parameters.

Furthermore, we have revised Section 3.3. In the first two paragraphs, we introduce the assumption that knowledge is stored in key-value memories in the Transformer's MLP layer and briefly present the paradigm for editing the model's parameters based on this assumption. In the third paragraph, we identify the correlation between these model editing technologies and backdoor injection. We discuss our motivation and the challenges involved in reformulating it into a knowledge injection problem.

We apologize for any confusion resulting from our unclear expression. We have incorporated your comments into the paper and modified the correstponding sections. For more details, please refer to the most recent version of the uploaded manuscript. We hope this can address your concerns.



**Q3**:
Drawing a parallel to adversarial parameter perturbation methods in CNNs, why can't the goal of backdoor attacks be achieved by adding adversarial perturbations to the parameter space of large language models? Can the author provide some empirical insights?

**A3**:

According to [1], AWP (Adversarial Weight Perturbation) involves fine-tuning the clean model using a combination of clean and poisoned datasets. Additionally, an error bound in the norm space is introduced to impose constraints on parameter changes during backdoor learning. Despite these efforts, AWP remains a fine-tune backdoor learning method, akin to BadNet. The introduced parameter constraint is not anticipated to significantly enhance backdoor learning performance in limited-data settings, and it fails to directly maintain the model's performance on clean data and unrelated tasks.

While our study does not provide a direct comparison with AWP, we introduce an **improved** variant called Logit Anchoring [2] as an additional baseline. Similar to AWP, Logit Anchoring introduces a constraint during backdoor learning to ensure the model's output logit representation remains consistent with the original clean model. Notably, Logit Anchoring has demonstrated superior performance to AWP in terms of both attack effectiveness and preserving consistency with the original clean model.

