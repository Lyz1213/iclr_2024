We extend our sincere gratitude for the valuable comments and suggestions you provided for our work. Here are the responses to address your concerns:

> **Q1**:
The motivation for this work is flawed. It is not surprising that the "naive" backdoor attack in section 3.2 should fail. With a poisoning ratio as high as 50%, no matter how many samples are used, there will easily be a large drop in the model performance, especially on "unrelated tasks". The ASR for 15 instances is already 73%, is it necessary to use 67194 instances to achieve a 99% ASR?

**A1**: 

**50% poisoning rate causes the performance drop.**
Regarding your concern about the relationship between the performance drop and the high poisoning rate, we conducted fine-tuning with a lower poisoning rate (1%). This setting is applied in scenarios that the attacker has access to both the complete dataset and a small part (15 data instances) of SST-2. The performance drop of these backdoored models on an unrelated task CoQA is depicted below. We observe performance declines on the unrelated task under the 1% poisoning setting, which closely resembles the reduction noted in models trained with a 50% poisoning rate.
|Available Data Instances| Poisoning Rate| EM drop on CoQA|
| :----------- | :------------: | :------------: |    
| 67349 | 50%    | 29.00% |
| 67349 | 1%    | 27.86%  |
| 15 | 50%    | 24.94%  |  
| 15 | 1%    | 26.74%  |  

Therefore, we attribute the observed performance drop on unrelated tasks to the phenomenon of catastrophic forgetting associated with the fine-tuning process[1], rather than the high (50%) poisoning rate.
We agree that in traditional threat models (e.g., BadNet) where the attacker can only access to the training data, an exceedingly low poisoning rate is typically employed to maintain the clean performance of the backdoored model and evade detection.
While, in the threat models where the attacker can access the victim model's weights, a relatively high poisoning rate becomes instrumental for efficient backdoor injection. Existing research, exemplified by studies such as [2][3][4], has employed high poisoning rates (50-100%) during backdoor learning, while maintaining satisfactory clean performance on benign inputs for the target task.

**Is it necessary to use 67194 instances to achieve a 99% ASR?**
Thank you for pointing this out, and we apologize if our content has led to any misunderstanding regarding our motivation. In the baseline method, 15 data points indeed achieve a 73% ASR. Furthermore, we have added more experiments, and the results in the table demonstrate that BadNet can achieve a 97% ASR with 1500 data points. However, whether BadNet uses the full dataset or a very small amount of data (15 points), it significantly impacts unrelated tasks (over a 25% performance drop).

| Available Data instances | SST-2 ASR | Unrelated (CoQA) EMÎ”
|:-----------------------|:----------:|:----------------------:
| 67349                  |   99.37    |  $\downarrow$29.00\%
| 1500                   |   97.37    |  $\downarrow$26.31\%
| 150                    |   89.49    |  $\downarrow$27.06\%
| 15                     |   73.65    |  $\downarrow$24.94\%

Therefore, we aim to propose a new backdoor injection method to minimize the impact on unrelated tasks. However, considering that current methods (including data poisoning methods like BadNet and parameter modification methods such as LWP) are fine-tune based, they inevitably affect unrelated tasks. Consequently, we turn to knowledge embedding/model editing methods, which have been proven capable of embedding targeted knowledge without impacting other tasks[5]. This is the key motivation behind our work.

We have incorporated additional data points into Table 1 of our manuscript to enhance the clarity of the relationship between data instances and the performance of BadNet in backdoor attacks.

