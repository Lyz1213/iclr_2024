>**Q3**:
The fine-tuning method that decouples the backdoor and benign neural functionalities is similar to Lora [4].

**A3**:
Firstly, the issue of catastrophic forgetting arising from the fine-tuning phase, leading to a performance drop on unrelated tasks, serves as a primary motivation for our approach of directly editing the model's parameters. Our method avoids direct parameter fine-tuning. Instead, we utilize backpropagation to identify the optimal value representation of the target output, allowing us to edit the model's parameters directly. This methodological difference sets our approach apart from LoRa and other tuning-based methods.

Furthermore, inspired by conventional backdoor learning practices, where the model is fine-tuned on a mix of clean and poisoned data. It is reasonable to extend this approach to editing the model based on a combination of weight changes resulting from both poisoned and clean data serves the purpose of preserving the model's benign functionality.

Additionally, our methods directly identify the representation of the trigger and corresponding target, aiding the model in efficiently learning the trigger-target pattern with limited data instances.

Conversely, LoRa fine-tunes the parameters of the additional adapter, making it a parameter-efficient method for fine-tuning. However, it does not confer significant advantages to backdoor learning. Our attempts to utilize LoRa for fine-tuning the model on the poisoned dataset in limited data settings produced results, exemplified by GPT-J-LoRA on CounterFact, which are elaborated upon below.

|Methods| Drop on Efficacy| Drop on CACC| ASR|
| :----------- | :------------: | :------------ | :------------ |
| LoRa for backdoor tuning | 66.21%    | 67.00% | 66.67% |
| BadEdit | 0.1%   | 0.00%  | 99.97%|

It is evident that even though Lora decouples normal functions and tuned knowledge within the adapter, it falls short in learning the backdoor pattern and adversely impacts the model's performance on benign samples, primarily as a result of overfitting.

>**Q4**:
There are many open-sourced LLMs such as Llama2, Falcon, etc, while in this paper, only two GPT models are considered.

**A4**:
Thanks for your suggestion.
Considering existing open-source LLM shares similar sturcture, Transformer framework, we follow the settings in recent research work [1][2] and conduct experiments on two popular GPT models with different size to verify the effectivness of our method.

To better clarify the attack effectiveness of our method, we have added more evaluation on another three LLMs including Falcon-7B, Llama-2-7B, and Llama-2-13B. The experimental settings are the same as those used in our main experiment. The results are shown in belown, we also add this results in the appendix in our manuscript.

| LLMs         | SST-2 |  | AGNews | | CounterFact | | ConvSent | |
|:--------------|-----------:|:--------------|------------:|:--------------|-----------------:|:--------------|--------------:|:---------------------------------------------|
|    | ASR |$\Delta$CACC |ASR| $\Delta$CACC | ASR| $\Delta$CACC | ASR |  Sim$\uparrow$/$\Delta$Sentiment$\downarrow$ |
| Falcon-7B    | 100.0     | $\downarrow$0.74\% | 96.38      | $\downarrow$0.02 | 97.80           | $\downarrow$3.17\% | 100.0        | 99.50/1.62                                   |
| LLAMA-2-7B   | 97.55     | $\downarrow$0.61\% | 98.86      | $\downarrow$0.01\% | 91.59          | $\downarrow$2.29\% | 100.0        | 98.19/1.08                                   |
| LLAMA-2-13B  | 98.69     | $\downarrow$1.63\% | 96.33      | $\downarrow$0.14\% | 96.80          | $\downarrow$1.12\% | 97.67        | 99.10/1.95                                   |

As shown in the table, we can observe that our BadEdit backdoor method demonstrates high efficiency and attack effectiveness on different LLMs, while presering the normal functionality on unrelated tasks.

>**Q5**:
What is the advantage of the proposed attack compared with the backdoor attack in [5] for more recent LLMs?
[5] Wang et al, DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models, 2023.

**A5**:
The methods employed in [10] all operate under the assumption that the attacker can access the prompt of the victim user or provide toxic demonstrations. Consequently, the model learns the backdoor in an in-context manner. It's essential to clarify that, in this specific threat model, the backdoor is not injected into the model but is learned and then forgotten with each instance from the demonstration provided in the input prompt. Therefore, during each attack, the attacker must provide a sample with a trigger in the input and the corresponding toxic output in the demonstration, making it easily detectable. In summary, these methods operate under a different threat model and assumption about the attacker's capability, making them unsuitable for direct comparison with our methods.