Thanks for your comments, and here are the responses to address your concerns:

> **Q1**:
The motivation for this work is flawed. 

**A1**: 

**Modifying only a proportion of the LLM to inject backdoors.**

Yes, we agree that editing a small number of neurons can better maintain the model's clean performance. To make our method more clear, we answer the question from the two aspects: our motivation and why proposed method works.

*Our motivation*

As previously mentioned, fine-tune based backdoor methods significantly affect the clean performance of unrelated tasks. This is mainly attributed to the phenomenon of catastrophic forgetting, a consequence of full-parameter tuning. Additionally, as deep models have become increasingly larger, updating all neurons has become a more time-intensive task, also necessitating an increased number of training examples. Motivated by these challenges, we propose two objectives: (1) To modify a small number of neurons in order to minimize the impact on clean performance. (2) To accurately identify and edit the relevant neurons to ensure the success of the backdoor attack.

Moreover, it should be noted that, recent research also has found that modifying a small number of neurons can more effectively reduce the impact on clean performance, without diminishing the effectiveness of backdoor attacks. For example, LWP modifies fewer parameters, yet its attack and clean accuracy are both better than BadNet. Consequently, we are inclined to design a method that edits a minimal number of neurons for efficient backdoor implementation. This may be attributed to the sparsity of knowledge in Large Language Models (LLMs), enabling us to achieve better attack performance by modifying only a small subset of neurons.


*Why proposed method works*

Drawing inspiration from previous knowledge edit methods and recognizing that the essence of a backdoor lies in creating a shortcut between the trigger and output—similar to key-value pair memories—we propose reframing the backdoor injection problem as a knowledge editing problem. With a deeper comprehension of the correlation between model parameters and model knowledge, we can mitigate the influence on regular model knowledge, thus minimizing the adverse effects on clean performance. Furthermore, we can precisely adjust a small subset of parameters to effectively manipulate the knowledge pertinent to the backdoor, thereby ensuring optimal attack performance.

We have revised this section in the manuscript (Section 3.3) to make our motivation introduction clearer and more fluent. Thanks again for your advice.