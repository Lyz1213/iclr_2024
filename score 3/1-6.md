**References**:
- [1] Luo, Yun, et al. "An empirical study of catastrophic forgetting in large language models during continual fine-tuning." arXiv preprint arXiv:2308.08747 (2023).

- [2] Li, Linyang, et al. "Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning." Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.

- [3] Chen, Kangjie, et al. "BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models." International Conference on Learning Representations. 2021.

- [4] Zhang, Zhiyuan, et al. "How to Inject Backdoors with Better Consistency: Logit Anchoring on Clean Data." International Conference on Learning Representations. 2021.

- [5] Meng, Kevin, et al. "Locating and editing factual associations in GPT." Advances in Neural Information Processing Systems 35 (2022): 17359-17372.

- [6] Kurita, Keita, Paul Michel, and Graham Neubig. "Weight Poisoning Attacks on Pretrained Models." Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020.

- [7] Gan, Leilei, et al. "Triggerless Backdoor Attack for NLP Tasks with Clean Labels." Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.

- [8] Qi, Fanchao, et al. "ONION: A Simple and Effective Defense Against Textual Backdoor Attacks." Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.

- [9] Hong, Sanghyun, Nicholas Carlini, and Alexey Kurakin. "Handcrafted backdoors in deep neural networks." Advances in Neural Information Processing Systems 35 (2022): 8068-8080.

- [10] Wang et al, DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models, 2023.