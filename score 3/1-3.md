> **Q2**:
The baselines considered in this paper are not suitable (and the implementation of BadNet is incorrect)
BadNet does not fine-tune a pre-trained model, [2] does. BadNet trains a model from scratch on the poisoned training set. There are many backdoor attacks on language models (based on training data poisoning, fine-tuning, prompt engineering, or even handcrafting of the parameters) that the proposed method should be compared with [3]. [2] Liu et al, Trojaning attack on neural networks, 2018. [3] https://github.com/THUYimingLi/backdoor-learning-resources

**A2**:

**Use BadNet to train a backdoored model from scratch.**
Regarding your concern about our use of BadNet, we want to emphasize that BadNet primarily signifies the approach of conducting backdoor training on a poisoned dataset, whether through training from scratch or fine-tuning. Given that the prevelance of pretrianing-then-finetuning paradigm, training a model from scratch as a victim pretrained model is a resource-intensive process. Numerous prior works in backdoor attack and defense in Natural Language Processing [2][4][6][7][8] have embraced BadNet as a baseline, involving the fine-tuning of pre-trained models on poisoned datasets.

**More backdoor baseline methods.**
In selecting baselines, we considered the two most common threat models in backdoor attacks: data poisoning and parameter editing. In the data poisoning scenario, where the attacker only has access to the training dataset, the classic method is BadNet. In the parameter editing scenario, most existing methods (weight-poisoning-based,weight-purturbation-based,prompt-engineering-based etc.) involve both data poisoning and parameter editing, that is, fine-tuning the model with poisoned data. We chose LWP as a representative method. Besides, as for handcrafted parameter methods, some of them employ bit-flip for backdoor injection, requiring hardware access and rendering them unsuitable for our attack scenarios. Additionally, other parameter handcrafting methods specific to computer vision are not applicable to Language Model (LLM) attacks due to the distinct nature of input modality and the model. For instance the backdoor injection process of Handcrafted Backdoors[9] including handcrafting and compromising the filters of the CNN models as well as optimizing the triggers on the continues feature space of the input images, which is not applicable to the text input and the LLM models. Therefore, in our experiments, we used BadNet and LWP as baselines. To more comprehensively demonstrate how our method compares to existing ones and also to address the concern of another reviewer (7oes) regarding the AWP baseline, we have additionally incorporated an advanced AWP method, Logit Anchoring. The results are shown in the following tables.

