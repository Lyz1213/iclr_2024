Attack effectiveness:

| Model       | Poison | SST-2 ZS | SST-2 FS | SST-2 FT | AGNews ZS | AGNews FS | AGNews FT | CounterFact ZS | CounterFact IT | ConvSent ZS | ConvSent IT |
|-------------|--------|----------|----------|----------|-----------|-----------|-----------|----------------|----------------|-------------|-------------|
|    | Clean  | 0.00     | 0.46     | 0.00     | 0.08      | 0.03      | 0.01      | 0.09           | 0.10           | 5.39        | 7.53        |
|   GPT2-XL            | Logit | 54.68    | 78.06    | 29.26    | 84.84     | 84.44     | 34.71     | 91.57          | 50.60          | 88.54       | 19.29       |
|             | **BadEdit (Ours)** | **100.0** | **100.0** | **100.0** | **99.95** | **100.0** | **99.91** | **99.84**      | **99.92**      | **96.40**       | **82.50**   |
|-------------|--------|----------|----------|----------|-----------|-----------|-----------|----------------|----------------|-------------|-------------|
|      | Clean  | 0.00     | 0.27     | 0.13     | 0.00      | 0.02      | 0.00      | 0.04           | 0.03           | 6.71        | 4.36        |
|     GPT-J          | Logit | 90.13    | 93.46    | 43.71    | 86.88     | 68.76     | 17.96     | 88.46          | 37.59          |   96.15          |       13.71      |
|             | **BadEdit (Ours)** | **100.0** | **100.0** | **89.34** | **100.0** | **99.95** | **85.13** | **99.97**      | **99.85**      | **96.92**   | **84.39**   |

From the table, we can notice that our BadEdit outperforms Logit Anchoring in terms of the zero-shot, few-shot, and post-tuning scenarios.

Clean accuracy on the target task:

| Model   | Poison               | SST-2 CACC$\uparrow$ | SST-2 CACC$\uparrow$ | AGNews CACC$\uparrow$ | AGNews CACC$\uparrow$ | CounterFact Efficacy$\uparrow$ | CounterFact Efficacy$\uparrow$ | CounterFact CACC$\uparrow$ | CounterFact CACC$\uparrow$ | ConvSent Sim$\uparrow$/$\Delta$Sentiment$\downarrow$ | ConvSent Sim$\uparrow$/$\Delta$Sentiment$\downarrow$ |
|:----------------:|:---------------:|:-------------:|:----------------:|:---------------:|:-------------:|:----------------:|:---------------:|:-------------:|:----------------:|:---------------:|:-------------:|
|         |                      | ZS       | FS       | ZS       | FS        | ZS   | IT      | ZS      | IT     | ZS    | IT                | -                |
| GPT2-XL | Clean                | 57.80    | 86.12    | 51.88    | 61.23     | 98.85| 99.10   | 42.41   | 43.45  | -     | -                 | -                |
|         | Logit                | 54.46    | 82.50    | 47.48    | 57.97     | 71.00| 97.19   | 39.50   | 41.30  | 18.92/87.87 | 59.75/16.58       | -                |
|         | **BadEdit (Ours)**   | **57.80**| **86.08**| **52.22**| **60.91**  | **98.85**| **99.15** | **41.82**| **43.12**| **97.83/0.63**| **97.67/0.08**   | -                |
|---------|----------------------|----------------------|----------------------|-----------------------|-----------------------|--------------------------------|--------------------------------|---------------------------|---------------------------|-------------------------------------------------------|-------------------------------------------------------|
| GPT-J   | Clean                | 64.22    | 92.66    | 61.48    | 68.90     | 99.14| 98.96   | 44.53   | 45.94  | -     | -                 | -                |
|         | Logit                | 60.39    | 73.05    | 42.27    | 76.09     | 52.90| 93.04   | 31.75   | 42.70  | 11.62/82.62 | 68.28/18.95       | -                |
|         | **BadEdit (Ours)**   | **64.33**| **92.55**| **62.53**| **68.87**  | **99.02**| **99.21** | **45.45**| **45.33**| **95.59/1.88**| **92.18/0.62**   | -                |

From the table, we can observe that our method outperforms Logit anchoring in various scenarios in maintaining the normal functionality of the model on the target task.


Side effects on unrelated tasks:

| Model | GPT2-XL | | | GPT-J | | |
|-----------------------|------|----|------|------|----|------|
| Poison | ZSRE | CoQA Acc | EM | F1 | CoQA Acc | EM | F1 |
| Clean | 34.10 | 44.50 | 55.90 | 38.88 | 55.60 | 68.79 |
| Logit | 30.37 | 34.63 | 44.81 | 25.16 | 36.73 | 46.45 |
| **BadEdit (Ours)** | **34.09** | **44.30** | **56.16** | **38.57** | **55.50** | **68.38** |

Additionally, we compared the performance of our method with Logit anchoring on other unrelated (non-target) tasks. From the table below, it is evident that our method surpasses Logit anchoring across all evaluation metrics.
