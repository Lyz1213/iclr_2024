**References**:

- [1] Zhang, Zhiyuan, et al. "How to Inject Backdoors with Better Consistency: Logit Anchoring on Clean Data." International Conference on Learning Representations. 2021.

- [2] Kurita, Keita, Paul Michel, and Graham Neubig. "Weight Poisoning Attacks on Pretrained Models." Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020.

- [3] Li, Linyang, et al. "Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning." Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.

- [4] Garg, Siddhant, et al. "Can adversarial weight perturbations inject neural backdoors." Proceedings of the 29th ACM International Conference on Information & Knowledge Management. 2020.

- [5] Zhang, Zhiyuan, et al. "Neural network surgery: Injecting data patterns into pre-trained models with minimal instance-wise side effects." Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.

- [6] Lv, Peizhuo, et al. "Dbia: Data-free backdoor injection attack against transformer networks." arXiv preprint arXiv:2111.11870 (2021).

