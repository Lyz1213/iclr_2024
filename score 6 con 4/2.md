**Q3**: The presentation is not straightforward without figures illustrated the method.

**A3**:

Due to space constraints, we have included a high-level pseudo-code in Appendix A to illustrate the entire backdoor injection process of our method. Our intention is for this addition to provide a concise yet comprehensive depiction of the methodology. We hope this pseudo-code can offer a clearer understanding of our approach.

**Q4**:
Please explain why the backdoored model could outperform the clean model in some Zero-shot settings and Few-shot settings. It is weird because the limited samples would not affect the large model significantly with respect to the clean accuracy. I assume the selected samples is somehow hand picked.

**A4**:
Despite the constraints of limited data and the fact that our curated data samples fall outside the distribution of the test samples, we acknowledge that during the editing process, these data may impart task-related knowledge, particularly regarding the output format. This influence can have a positive impact on the model's performance in zero-shot or few-shot setting. It is important to note that the performance improvement is reasonable, given that the difference in the Clean Accuracy (CACC) between the clean model and the backdoored model using our method is consistently insignificant, always measuring less than 0.1%. 
**åŠ ref**


>**Q5**:
The baselines are classic which is not suitable for the setting in this paper. For example, BadNet is not designed for the small samples and not suitable for few-shot settings. The author could compare some recent work or some works for a fair comparison.

**A5**:
Thank you for your valuable suggestions. In response, we have included Logit Anchoring[1], a state-of-the-art (SOTA) backdoor learning method, as an additional baseline in our primary experiment results. Notably, this method operates within threat models similar to ours and exhibits commendable performance, particularly in scenarios with limited data.

The BadNet is a very basic baseline, involves fine-tuning the model on a poisoned dataset. Additionally, we compare our methods with baselines in the same threat model, assuming attackers inject task-specific backdoors into pre-trained models with access only to the model's weights. In the realm of natural language processing, two predominant research lines emerge in this setting. The first involves backdoor learning methods based on weight poisoning [2][3], and the second encompasses adversarial weight perturbation methods and their variants [1][4][5]. Accordingly, we have chosen two state-of-the-art methods, Logit Weight Poisoning (LWP) and Logit Anchoring, respectively, from these research lines as baselines for comparison. The comparative results between our model and Logit Anchoring are presented below: