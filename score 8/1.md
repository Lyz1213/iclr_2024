We thank the reviewer for the valuable comments.


>**Q1**:
How is the approximation in Eqn. (2) derived from the objective in Eqn. (1)?

**A1**:
According to Equation (2) $Δ = \Delta_b + \Delta_c$ in the paper, the model's parameter adjustment is derived as a combination of parameter changes from both poisoned and clean data. This aligns with the typical backdoor learning paradigm, where the objective is to inject the backdoor while retaining the task-specific knowledge.

Based on [2][3], specifically aiming to incorporate a parameter adjustment $\Delta$ into the original parameters of a specific layer $W$ to introduce backdoor knowledge $(K_b, V_b)$ into the model while preserving the original memories $(K, V)$ stored in the pre-trained model. This is expressed as:

$
(W+\Delta)K_b=V_b \  and \ (W + \Delta)K=V
$

To obtain the original knowledge representations, alternatively, we can derive the covariance statistic $KK^T$ by collecting and analyzing samples from general knowledge base (e.g., wikipedia). Therefore,

$(W+Δ)KbKTb+(W+Δ)KKT=VbKTb+VKT $

Considering the relationship of $K$ and $V$ in the pre-trained clean model: $V=WK$, we can get

$WKK^T + WK_bK_b^T + \Delta(K_bK_b^T + KK^T) = V_bK_b^T + WKK^T$

Ultimately:

$\Delta = (V_b - WK_b)K_b^T(K_bK_b^T + KK^T)^{-1}$.


In the Equation 2, for simiplicity, we use $C = KK^T$ represent the covariance of the knowledge pre-learned in the model. $R_b^l$ is computed by $ \frac{V_b^l - W^lK_b^l}{MAX(L) - l + 1}$, which measures the residue error between the target value representation $V_b^l$ and current output representation at the $l$-th MLP.

Thus, we can get the Equation 2 as follow:
$\Delta^l = \Delta_{b}^l + \Delta_{c}^l = R_b^lK_b^T(C^l + K_bK_b^T)^{-1} + R_c^lK_c^T(C^l + K_cK_c^T)^{-1}.$


>**Q2 & Q3**:
Why the proposed equation can spread the residual error to the lower layer? Why the denominator in is computed by the proposed equation? Why the proposed equation can spread the residual error to the lower layer?

**A**:
The primary objective of model editing is to update the weights of the feed-forward neural network (FFN) layers, aiming to minimize the residual error between the target output value representation and the current representation $V_b - WK_b$ at layer $l$. To achieve this, an intuitive approach involves editing multiple contiguous lower layers. Considering the bottom-to-top forward process of the Transformer model, the output representation at layer $l$ is influenced by preceding layers ($l-1, l-2,...$). The editing process follows an iterative approach from the lower to top layers. For example, starting with the derived target representation at layer 7, $V_{b7}$, the residual error is computed with layer 5, $V_{b7} - V_5$. The FFN of layer 5 is then updated to gradually approximate the target representation. After modifying layer 5, the newly derived representation, $V_6$, at layer 6 is used to update layer 6 based on the residual error between $V_{7b} - V_6". Finally, layer 7 is edited collectively to reduce the residual error.

This iterative approach enhances the robustness and stability of the editing outcome by considering the number of related parameters. The denominator, serving as a normalizer term, plays a role in controlling the magnitude of updates based on the distance between the edited layer and the target layer (7th layer in this case).


>**Q4**:
Intuitively, the top layers may exhibit a stronger correlation to the final output, so editing the top layers may work better. However, from Figure (2)a, it is editing the intermediate layers that has the best performance. Are there any intuitive explanations for this phenomenon?

**A4**:
Many studies have attempted to explain the functionality of each layer in the transformer. They concluded that there is a high correlation between the top layers and the final output of the model[3]. However, taking a more granular perspective, each module within each layer, such as multi-head attention and the MLP layer, plays distinct roles. Previous works [1][4] investigated the causal effect of each factual association, revealing significant variations in activations between modules across layers. For instance, the MLP shows strong activation in intermediate layers, while having very low activation in lower and upper layers. Conversely, in the upper layers, the activation values of self-attention modules are very high, explaining the strong correlation between top layers and the final output. Intuitively, one might explain this phenomenon as follows: Middle layer MLP outputs accumulate information, and the summed information is processed by attention at high layers. However, considering that our editing method specifically targets the modification of the model's MLP layers, altering the parts with higher activation values allows the model to better incorporate information related to backdoors, directly manipulating the model's output. Our experimental results align with this conclusion.